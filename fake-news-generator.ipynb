{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"156e42870f5940b68b2aed9c071d8379":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d02d8dad47240e5992f263d56d0b07b","IPY_MODEL_1ff1d17d4e1b4ea2a708ab3ccfe8e19a","IPY_MODEL_aaa5f0be907d4b01a8ea5ee08afb2345"],"layout":"IPY_MODEL_d52d65bf97774f138e927007de74d652"}},"3d02d8dad47240e5992f263d56d0b07b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3717a09c0f94e00b4691bd9e095139b","placeholder":"​","style":"IPY_MODEL_dcbfc8112a5a465bb4de567649b88033","value":"Map: 100%"}},"1ff1d17d4e1b4ea2a708ab3ccfe8e19a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1be0b39375a14811990c033d80f2dc87","max":226711,"min":0,"orientation":"horizontal","style":"IPY_MODEL_542e3b9878c44046b21c79a40c41d8f4","value":226711}},"aaa5f0be907d4b01a8ea5ee08afb2345":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6ca1acc50df48438391784525f1b7d3","placeholder":"​","style":"IPY_MODEL_be517cfb8019460eb59fbe851e56277b","value":" 226711/226711 [09:12&lt;00:00, 364.79 examples/s]"}},"d52d65bf97774f138e927007de74d652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3717a09c0f94e00b4691bd9e095139b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcbfc8112a5a465bb4de567649b88033":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1be0b39375a14811990c033d80f2dc87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"542e3b9878c44046b21c79a40c41d8f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6ca1acc50df48438391784525f1b7d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be517cfb8019460eb59fbe851e56277b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Importing dependencies","metadata":{"id":"dj9cRcCpYs8Y"}},{"cell_type":"code","source":"!pip install accelerate --upgrade","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQF_5OxqrDh_","outputId":"8c398707-d2e4-4647-a9e6-453687e29c63","execution":{"iopub.status.busy":"2024-01-07T20:46:52.462257Z","iopub.execute_input":"2024-01-07T20:46:52.463099Z","iopub.status.idle":"2024-01-07T20:47:06.145904Z","shell.execute_reply.started":"2024-01-07T20:46:52.463049Z","shell.execute_reply":"2024-01-07T20:47:06.144519Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets --upgrade","metadata":{"id":"l2430k_6x1d9","execution":{"iopub.status.busy":"2024-01-07T20:47:06.148239Z","iopub.execute_input":"2024-01-07T20:47:06.148578Z","iopub.status.idle":"2024-01-07T20:47:18.785109Z","shell.execute_reply.started":"2024-01-07T20:47:06.148548Z","shell.execute_reply":"2024-01-07T20:47:18.783996Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --upgrade","metadata":{"id":"P6nJBxf2xP9p","execution":{"iopub.status.busy":"2024-01-07T20:47:18.786826Z","iopub.execute_input":"2024-01-07T20:47:18.787197Z","iopub.status.idle":"2024-01-07T20:47:24.495642Z","shell.execute_reply.started":"2024-01-07T20:47:18.787163Z","shell.execute_reply":"2024-01-07T20:47:24.494146Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np","metadata":{"id":"854fdkhrY2cO","execution":{"iopub.status.busy":"2024-01-07T20:47:24.503303Z","iopub.execute_input":"2024-01-07T20:47:24.503875Z","iopub.status.idle":"2024-01-07T20:47:24.512163Z","shell.execute_reply.started":"2024-01-07T20:47:24.503834Z","shell.execute_reply":"2024-01-07T20:47:24.511510Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{"id":"zymsePFsZBrO"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"EdinburghNLP/xsum\")","metadata":{"id":"A6sj0xP7Y-vc","execution":{"iopub.status.busy":"2024-01-07T20:47:24.512968Z","iopub.execute_input":"2024-01-07T20:47:24.513229Z","iopub.status.idle":"2024-01-07T20:47:27.233186Z","shell.execute_reply.started":"2024-01-07T20:47:24.513195Z","shell.execute_reply":"2024-01-07T20:47:27.232060Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"id":"5_ixDESDZgCZ","execution":{"iopub.status.busy":"2024-01-07T20:47:27.234750Z","iopub.execute_input":"2024-01-07T20:47:27.235059Z","iopub.status.idle":"2024-01-07T20:47:27.243564Z","shell.execute_reply.started":"2024-01-07T20:47:27.235032Z","shell.execute_reply":"2024-01-07T20:47:27.242629Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'train': (204045, 3), 'validation': (11332, 3), 'test': (11334, 3)}"},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"id":"MjL9stNmZIGO","execution":{"iopub.status.busy":"2024-01-07T20:47:27.244851Z","iopub.execute_input":"2024-01-07T20:47:27.245172Z","iopub.status.idle":"2024-01-07T20:47:27.255893Z","shell.execute_reply.started":"2024-01-07T20:47:27.245147Z","shell.execute_reply":"2024-01-07T20:47:27.254764Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11334\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Concatenate the validation and test sets to the training set\ncombined_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n\n# Print the shape of the combined dataset\nprint(combined_dataset.shape)\n","metadata":{"id":"W_2nNcFnLKuG","execution":{"iopub.status.busy":"2024-01-07T20:47:27.257246Z","iopub.execute_input":"2024-01-07T20:47:27.257547Z","iopub.status.idle":"2024-01-07T20:47:27.274051Z","shell.execute_reply.started":"2024-01-07T20:47:27.257522Z","shell.execute_reply":"2024-01-07T20:47:27.272876Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"(226711, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_dataset","metadata":{"id":"BYR6PGYnRuRg","execution":{"iopub.status.busy":"2024-01-07T20:47:27.275371Z","iopub.execute_input":"2024-01-07T20:47:27.276138Z","iopub.status.idle":"2024-01-07T20:47:27.283856Z","shell.execute_reply.started":"2024-01-07T20:47:27.276104Z","shell.execute_reply":"2024-01-07T20:47:27.281771Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['document', 'summary', 'id'],\n    num_rows: 226711\n})"},"metadata":{}}]},{"cell_type":"code","source":"import random\n\nrandom.seed(42)\nsample_size = 22000\nsample_indices = random.sample(range(len(combined_dataset)), sample_size)\nsampled_dataset = combined_dataset.select(sample_indices)","metadata":{"id":"HlWbQos-OOc0","execution":{"iopub.status.busy":"2024-01-07T20:47:27.289606Z","iopub.execute_input":"2024-01-07T20:47:27.289906Z","iopub.status.idle":"2024-01-07T20:47:27.444070Z","shell.execute_reply.started":"2024-01-07T20:47:27.289881Z","shell.execute_reply":"2024-01-07T20:47:27.442994Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"sampled_dataset","metadata":{"id":"ng6GNubeOWaw","execution":{"iopub.status.busy":"2024-01-07T20:47:27.445318Z","iopub.execute_input":"2024-01-07T20:47:27.445660Z","iopub.status.idle":"2024-01-07T20:47:27.453540Z","shell.execute_reply.started":"2024-01-07T20:47:27.445627Z","shell.execute_reply":"2024-01-07T20:47:27.452459Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['document', 'summary', 'id'],\n    num_rows: 22000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"#Model","metadata":{"id":"9WtpOU19ZyS0"}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name,)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"S9fzEGxgZgW9","execution":{"iopub.status.busy":"2024-01-07T20:47:27.454664Z","iopub.execute_input":"2024-01-07T20:47:27.455015Z","iopub.status.idle":"2024-01-07T20:47:35.626877Z","shell.execute_reply.started":"2024-01-07T20:47:27.454974Z","shell.execute_reply":"2024-01-07T20:47:35.625707Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    # Get the name of the GPU\n    device = torch.cuda.get_device_name(0)\n    print(f'T5 model is running on GPU: {device}')\nelse:\n    print('T5 model is running on CPU')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3PSIieWeZ7Zm","outputId":"1ee74d94-8253-43ae-d27d-7e00c20a2f15","execution":{"iopub.status.busy":"2024-01-07T20:47:35.628557Z","iopub.execute_input":"2024-01-07T20:47:35.630001Z","iopub.status.idle":"2024-01-07T20:47:35.639355Z","shell.execute_reply.started":"2024-01-07T20:47:35.629963Z","shell.execute_reply":"2024-01-07T20:47:35.638303Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"T5 model is running on GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vk1lfOiaAT1","outputId":"03329a9c-2456-4091-f952-d3f0f9e8515f","execution":{"iopub.status.busy":"2024-01-07T20:47:35.640631Z","iopub.execute_input":"2024-01-07T20:47:35.641343Z","iopub.status.idle":"2024-01-07T20:47:35.654535Z","shell.execute_reply.started":"2024-01-07T20:47:35.641304Z","shell.execute_reply":"2024-01-07T20:47:35.653472Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Tokenize Data","metadata":{"id":"u0jOQdEhagQm"}},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Generate news article about the following Text.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"summary\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"document\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n    return example","metadata":{"id":"bl-S2h1gaEsr","execution":{"iopub.status.busy":"2024-01-07T20:47:35.655909Z","iopub.execute_input":"2024-01-07T20:47:35.656194Z","iopub.status.idle":"2024-01-07T20:47:35.666842Z","shell.execute_reply.started":"2024-01-07T20:47:35.656169Z","shell.execute_reply":"2024-01-07T20:47:35.665750Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = sampled_dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id','document', 'summary',])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["156e42870f5940b68b2aed9c071d8379","3d02d8dad47240e5992f263d56d0b07b","1ff1d17d4e1b4ea2a708ab3ccfe8e19a","aaa5f0be907d4b01a8ea5ee08afb2345","d52d65bf97774f138e927007de74d652","a3717a09c0f94e00b4691bd9e095139b","dcbfc8112a5a465bb4de567649b88033","1be0b39375a14811990c033d80f2dc87","542e3b9878c44046b21c79a40c41d8f4","f6ca1acc50df48438391784525f1b7d3","be517cfb8019460eb59fbe851e56277b"]},"id":"I4dAAu6ncj1f","outputId":"2fa09cf0-e52f-4255-f979-c1b559efbe28","execution":{"iopub.status.busy":"2024-01-07T20:47:35.668259Z","iopub.execute_input":"2024-01-07T20:47:35.668666Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef5141c3ef37415b902a00a3c36db22c"}},"metadata":{}}]},{"cell_type":"markdown","source":"#Hugging Face","metadata":{"id":"EGufUTCkdB8m"}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmfiVk9-dERd","outputId":"f371ab5e-c66c-4b6b-9ffa-5fe3c644e162","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Training Arguments","metadata":{"id":"z116COR9c2vu"}},{"cell_type":"code","source":"import os\n\n# define the name of the directory to be created\npath = \"/kaggle/working/t5\"\n\ntry:\n    os.mkdir(path)\nexcept OSError:\n    print(f\"Creation of the directory {path} failed\")\nelse:\n    print(f\"Successfully created the directory {path}\")\n","metadata":{"id":"GtaFGZJiR_S-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=path,\n    push_to_hub=True,\n    push_to_hub_model_id='Fake-news-gen',\n    push_to_hub_token='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',\n    logging_strategy=\"epoch\",\n    #evaluation_strategy=\"steps\",\n    num_train_epochs=10,\n    auto_find_batch_size=True,\n    #bf16=True,\n    #eval_steps=500,\n    save_total_limit=3,\n)\n\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets\n\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGgA281WcsGe","outputId":"aaa9433c-13f4-424b-ae4a-031c4cf3671e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"OY1gLUy6lTEN","outputId":"8d28a8a9-6c1d-43ce-c306-5222b1841532","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"id":"FGA5SeB_z_Fi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"Fake-news-gen\")\ntokenizer.push_to_hub(\"Fake-news-gen\")","metadata":{"id":"_teoUFHwnik1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained('Ahmedhany216/Fake-news-gen')\ntokenizer = AutoTokenizer.from_pretrained('Ahmedhany216/Fake-news-gen')","metadata":{"id":"ZeIPFIl91G2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ndef generate_article(input_text, model_name, max_length=500):\n    start_prompt = 'Generate news article about the following Text.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    # Load pre-trained model and tokenizer\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Concatenate the prompts\n    prompt = f'{start_prompt}{input_text}{end_prompt}'\n\n    # Tokenize the input text\n    input_ids = tokenizer.encode(prompt, return_tensors='pt', max_length=max_length, truncation=True)\n\n    # Generate article\n    output = model.generate(input_ids, max_length=max_length, num_beams=5,do_sample=tr ,length_penalty=0.6, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n\n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\n\n# Example usage\ninput_text = \"the persident was killed in the conference\"\ngenerated_article = generate_article(input_text, 'Ahmedhany216/Fake-news-gen')\nprint(generated_article)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}